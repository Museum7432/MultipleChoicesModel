{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arch/Projects/college/semeval_brainteaser\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "usage: main.py [options] fit [-h] [-c CONFIG] [--print_config[=flags]]\n",
      "                             [--seed_everything SEED_EVERYTHING]\n",
      "                             [--trainer CONFIG]\n",
      "                             [--trainer.accelerator.help CLASS_PATH_OR_NAME]\n",
      "                             [--trainer.accelerator ACCELERATOR]\n",
      "                             [--trainer.strategy.help CLASS_PATH_OR_NAME]\n",
      "                             [--trainer.strategy STRATEGY]\n",
      "                             [--trainer.devices DEVICES]\n",
      "                             [--trainer.num_nodes NUM_NODES]\n",
      "                             [--trainer.precision PRECISION]\n",
      "                             [--trainer.logger.help CLASS_PATH_OR_NAME]\n",
      "                             [--trainer.logger LOGGER]\n",
      "                             [--trainer.callbacks.help CLASS_PATH_OR_NAME]\n",
      "                             [--trainer.callbacks CALLBACKS]\n",
      "                             [--trainer.fast_dev_run FAST_DEV_RUN]\n",
      "                             [--trainer.max_epochs MAX_EPOCHS]\n",
      "                             [--trainer.min_epochs MIN_EPOCHS]\n",
      "                             [--trainer.max_steps MAX_STEPS]\n",
      "                             [--trainer.min_steps MIN_STEPS]\n",
      "                             [--trainer.max_time MAX_TIME]\n",
      "                             [--trainer.limit_train_batches LIMIT_TRAIN_BATCHES]\n",
      "                             [--trainer.limit_val_batches LIMIT_VAL_BATCHES]\n",
      "                             [--trainer.limit_test_batches LIMIT_TEST_BATCHES]\n",
      "                             [--trainer.limit_predict_batches LIMIT_PREDICT_BATCHES]\n",
      "                             [--trainer.overfit_batches OVERFIT_BATCHES]\n",
      "                             [--trainer.val_check_interval VAL_CHECK_INTERVAL]\n",
      "                             [--trainer.check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH]\n",
      "                             [--trainer.num_sanity_val_steps NUM_SANITY_VAL_STEPS]\n",
      "                             [--trainer.log_every_n_steps LOG_EVERY_N_STEPS]\n",
      "                             [--trainer.enable_checkpointing {true,false,null}]\n",
      "                             [--trainer.enable_progress_bar {true,false,null}]\n",
      "                             [--trainer.enable_model_summary {true,false,null}]\n",
      "                             [--trainer.accumulate_grad_batches ACCUMULATE_GRAD_BATCHES]\n",
      "                             [--trainer.gradient_clip_val GRADIENT_CLIP_VAL]\n",
      "                             [--trainer.gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM]\n",
      "                             [--trainer.deterministic DETERMINISTIC]\n",
      "                             [--trainer.benchmark {true,false,null}]\n",
      "                             [--trainer.inference_mode {true,false}]\n",
      "                             [--trainer.use_distributed_sampler {true,false}]\n",
      "                             [--trainer.profiler.help CLASS_PATH_OR_NAME]\n",
      "                             [--trainer.profiler PROFILER]\n",
      "                             [--trainer.detect_anomaly {true,false}]\n",
      "                             [--trainer.barebones {true,false}]\n",
      "                             [--trainer.plugins.help CLASS_PATH_OR_NAME]\n",
      "                             [--trainer.plugins PLUGINS]\n",
      "                             [--trainer.sync_batchnorm {true,false}]\n",
      "                             [--trainer.reload_dataloaders_every_n_epochs RELOAD_DATALOADERS_EVERY_N_EPOCHS]\n",
      "                             [--trainer.default_root_dir DEFAULT_ROOT_DIR]\n",
      "                             [--model CONFIG]\n",
      "                             [--model.encoder_name ENCODER_NAME]\n",
      "                             [--model.gradient_checkpointing GRADIENT_CHECKPOINTING]\n",
      "                             [--model.lr LR]\n",
      "                             [--model.use_last_hidden_state USE_LAST_HIDDEN_STATE]\n",
      "                             [--data CONFIG]\n",
      "                             [--data.train_data_path TRAIN_DATA_PATH]\n",
      "                             [--data.encoder_name ENCODER_NAME]\n",
      "                             [--data.promt_style {true,false}]\n",
      "                             [--data.shuffle_choices {true,false}]\n",
      "                             [--data.num_workers NUM_WORKERS]\n",
      "                             [--data.train_batch_size TRAIN_BATCH_SIZE]\n",
      "                             [--data.valid_batch_size VALID_BATCH_SIZE]\n",
      "                             [--data.debug {true,false}]\n",
      "                             [--optimizer.help CLASS_PATH_OR_NAME]\n",
      "                             [--optimizer CONFIG | CLASS_PATH_OR_NAME | .INIT_ARG_NAME VALUE]\n",
      "                             [--lr_scheduler.help CLASS_PATH_OR_NAME]\n",
      "                             [--lr_scheduler CONFIG | CLASS_PATH_OR_NAME | .INIT_ARG_NAME VALUE]\n",
      "                             [--ckpt_path CKPT_PATH]\n",
      "\n",
      "Runs the full optimization routine.\n",
      "\n",
      "options:\n",
      "  -h, --help            Show this help message and exit.\n",
      "  -c CONFIG, --config CONFIG\n",
      "                        Path to a configuration file in json or yaml format.\n",
      "  --print_config[=flags]\n",
      "                        Print the configuration after applying all other\n",
      "                        arguments and exit. The optional flags customizes the\n",
      "                        output and are one or more keywords separated by\n",
      "                        comma. The supported flags are: comments,\n",
      "                        skip_default, skip_null.\n",
      "  --seed_everything SEED_EVERYTHING\n",
      "                        Set to an int to run seed_everything with this value\n",
      "                        before classes instantiation.Set to True to use a\n",
      "                        random seed. (type: Union[bool, int], default: True)\n",
      "\n",
      "Customize every aspect of training via flags:\n",
      "  --trainer CONFIG      Path to a configuration file.\n",
      "  --trainer.accelerator.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of Accelerator\n",
      "                        and exit.\n",
      "  --trainer.accelerator ACCELERATOR\n",
      "                        Supports passing different accelerator types (\"cpu\",\n",
      "                        \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"mps\", \"auto\") as well as\n",
      "                        custom accelerator instances. (type: Union[str,\n",
      "                        Accelerator], default: auto, known subclasses:\n",
      "                        lightning.pytorch.accelerators.CPUAccelerator,\n",
      "                        lightning.pytorch.accelerators.CUDAAccelerator,\n",
      "                        lightning.pytorch.accelerators.MPSAccelerator,\n",
      "                        lightning.pytorch.accelerators.XLAAccelerator)\n",
      "  --trainer.strategy.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of Strategy and\n",
      "                        exit.\n",
      "  --trainer.strategy STRATEGY\n",
      "                        Supports different training strategies with aliases as\n",
      "                        well custom strategies. Default: ``\"auto\"``. (type:\n",
      "                        Union[str, Strategy], default: auto, known subclasses:\n",
      "                        lightning.pytorch.strategies.DDPStrategy,\n",
      "                        lightning.pytorch.strategies.DeepSpeedStrategy,\n",
      "                        lightning.pytorch.strategies.XLAStrategy,\n",
      "                        lightning.pytorch.strategies.FSDPStrategy,\n",
      "                        lightning.pytorch.strategies.SingleDeviceStrategy,\n",
      "                        lightning.pytorch.strategies.SingleDeviceXLAStrategy)\n",
      "  --trainer.devices DEVICES, --trainer.devices+ DEVICES\n",
      "                        The devices to use. Can be set to a positive number\n",
      "                        (int or str), a sequence of device indices (list or\n",
      "                        str), the value ``-1`` to indicate all available\n",
      "                        devices should be used, or ``\"auto\"`` for automatic\n",
      "                        selection based on the chosen accelerator. Default:\n",
      "                        ``\"auto\"``. (type: Union[List[int], str, int],\n",
      "                        default: auto)\n",
      "  --trainer.num_nodes NUM_NODES\n",
      "                        Number of GPU nodes for distributed training. Default:\n",
      "                        ``1``. (type: int, default: 1)\n",
      "  --trainer.precision PRECISION\n",
      "                        Double precision (64, '64' or '64-true'), full\n",
      "                        precision (32, '32' or '32-true'), 16bit mixed\n",
      "                        precision (16, '16', '16-mixed') or bfloat16 mixed\n",
      "                        precision ('bf16', 'bf16-mixed'). Can be used on CPU,\n",
      "                        GPU, TPUs, HPUs or IPUs. Default: ``'32-true'``.\n",
      "                        (type: Union[Literal[64, 32, 16],\n",
      "                        Literal['transformer-engine', 'transformer-engine-\n",
      "                        float16', '16-true', '16-mixed', 'bf16-true',\n",
      "                        'bf16-mixed', '32-true', '64-true'], Literal['64',\n",
      "                        '32', '16', 'bf16'], null], default: null)\n",
      "  --trainer.logger.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of Logger and\n",
      "                        exit.\n",
      "  --trainer.logger LOGGER, --trainer.logger+ LOGGER\n",
      "                        Logger (or iterable collection of loggers) for\n",
      "                        experiment tracking. A ``True`` value uses the default\n",
      "                        ``TensorBoardLogger`` if it is installed, otherwise\n",
      "                        ``CSVLogger``. ``False`` will disable logging. If\n",
      "                        multiple loggers are provided, local files\n",
      "                        (checkpoints, profiler traces, etc.) are saved in the\n",
      "                        ``log_dir`` of the first logger. Default: ``True``.\n",
      "                        (type: Union[Logger, Iterable[Logger], bool, null],\n",
      "                        default: null, known subclasses:\n",
      "                        lightning.pytorch.loggers.logger.DummyLogger,\n",
      "                        lightning.pytorch.loggers.CometLogger,\n",
      "                        lightning.pytorch.loggers.CSVLogger,\n",
      "                        lightning.pytorch.loggers.MLFlowLogger,\n",
      "                        lightning.pytorch.loggers.NeptuneLogger,\n",
      "                        lightning.pytorch.loggers.TensorBoardLogger,\n",
      "                        lightning.pytorch.loggers.WandbLogger)\n",
      "  --trainer.callbacks.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of Callback and\n",
      "                        exit.\n",
      "  --trainer.callbacks CALLBACKS, --trainer.callbacks+ CALLBACKS\n",
      "                        Add a callback or list of callbacks. Default:\n",
      "                        ``None``. (type: Union[List[Callback], Callback,\n",
      "                        null], default: null, known subclasses:\n",
      "                        lightning.Callback,\n",
      "                        lightning.pytorch.callbacks.BatchSizeFinder,\n",
      "                        lightning.pytorch.callbacks.Checkpoint,\n",
      "                        lightning.pytorch.callbacks.ModelCheckpoint,\n",
      "                        lightning.pytorch.callbacks.OnExceptionCheckpoint,\n",
      "                        lightning.pytorch.callbacks.DeviceStatsMonitor,\n",
      "                        lightning.pytorch.callbacks.EarlyStopping,\n",
      "                        lightning.pytorch.callbacks.BaseFinetuning,\n",
      "                        lightning.pytorch.callbacks.BackboneFinetuning, lightn\n",
      "                        ing.pytorch.callbacks.GradientAccumulationScheduler,\n",
      "                        lightning.pytorch.callbacks.LambdaCallback,\n",
      "                        lightning.pytorch.callbacks.LearningRateFinder,\n",
      "                        lightning.pytorch.callbacks.LearningRateMonitor,\n",
      "                        lightning.pytorch.callbacks.ModelSummary,\n",
      "                        lightning.pytorch.callbacks.RichModelSummary,\n",
      "                        lightning.pytorch.callbacks.BasePredictionWriter,\n",
      "                        lightning.pytorch.callbacks.ProgressBar,\n",
      "                        lightning.pytorch.callbacks.RichProgressBar,\n",
      "                        lightning.pytorch.callbacks.TQDMProgressBar,\n",
      "                        lightning.pytorch.callbacks.Timer,\n",
      "                        lightning.pytorch.callbacks.ModelPruning,\n",
      "                        lightning.pytorch.callbacks.SpikeDetection,\n",
      "                        lightning.pytorch.callbacks.StochasticWeightAveraging,\n",
      "                        lightning.pytorch.cli.SaveConfigCallback)\n",
      "  --trainer.fast_dev_run FAST_DEV_RUN\n",
      "                        Runs n if set to ``n`` (int) else 1 if set to ``True``\n",
      "                        batch(es) of train, val and test to find any bugs (ie:\n",
      "                        a sort of unit test). Default: ``False``. (type:\n",
      "                        Union[int, bool], default: False)\n",
      "  --trainer.max_epochs MAX_EPOCHS\n",
      "                        Stop training once this number of epochs is reached.\n",
      "                        Disabled by default (None). If both max_epochs and\n",
      "                        max_steps are not specified, defaults to ``max_epochs\n",
      "                        = 1000``. To enable infinite training, set\n",
      "                        ``max_epochs = -1``. (type: Optional[int], default:\n",
      "                        null)\n",
      "  --trainer.min_epochs MIN_EPOCHS\n",
      "                        Force training for at least these many epochs.\n",
      "                        Disabled by default (None). (type: Optional[int],\n",
      "                        default: null)\n",
      "  --trainer.max_steps MAX_STEPS\n",
      "                        Stop training after this number of steps. Disabled by\n",
      "                        default (-1). If ``max_steps = -1`` and ``max_epochs =\n",
      "                        None``, will default to ``max_epochs = 1000``. To\n",
      "                        enable infinite training, set ``max_epochs`` to\n",
      "                        ``-1``. (type: int, default: -1)\n",
      "  --trainer.min_steps MIN_STEPS\n",
      "                        Force training for at least these number of steps.\n",
      "                        Disabled by default (``None``). (type: Optional[int],\n",
      "                        default: null)\n",
      "  --trainer.max_time MAX_TIME\n",
      "                        Stop training after this amount of time has passed.\n",
      "                        Disabled by default (``None``). The time duration can\n",
      "                        be specified in the format DD:HH:MM:SS (days, hours,\n",
      "                        minutes seconds), as a :class:`datetime.timedelta`, or\n",
      "                        a dictionary with keys that will be passed to\n",
      "                        :class:`datetime.timedelta`. (type: Union[str,\n",
      "                        timedelta, Dict[str, int], null], default: null)\n",
      "  --trainer.limit_train_batches LIMIT_TRAIN_BATCHES\n",
      "                        How much of training dataset to check (float =\n",
      "                        fraction, int = num_batches). Default: ``1.0``. (type:\n",
      "                        Union[int, float, null], default: null)\n",
      "  --trainer.limit_val_batches LIMIT_VAL_BATCHES\n",
      "                        How much of validation dataset to check (float =\n",
      "                        fraction, int = num_batches). Default: ``1.0``. (type:\n",
      "                        Union[int, float, null], default: null)\n",
      "  --trainer.limit_test_batches LIMIT_TEST_BATCHES\n",
      "                        How much of test dataset to check (float = fraction,\n",
      "                        int = num_batches). Default: ``1.0``. (type:\n",
      "                        Union[int, float, null], default: null)\n",
      "  --trainer.limit_predict_batches LIMIT_PREDICT_BATCHES\n",
      "                        How much of prediction dataset to check (float =\n",
      "                        fraction, int = num_batches). Default: ``1.0``. (type:\n",
      "                        Union[int, float, null], default: null)\n",
      "  --trainer.overfit_batches OVERFIT_BATCHES\n",
      "                        Overfit a fraction of training/validation data (float)\n",
      "                        or a set number of batches (int). Default: ``0.0``.\n",
      "                        (type: Union[int, float], default: 0.0)\n",
      "  --trainer.val_check_interval VAL_CHECK_INTERVAL\n",
      "                        How often to check the validation set. Pass a\n",
      "                        ``float`` in the range [0.0, 1.0] to check after a\n",
      "                        fraction of the training epoch. Pass an ``int`` to\n",
      "                        check after a fixed number of training batches. An\n",
      "                        ``int`` value can only be higher than the number of\n",
      "                        training batches when\n",
      "                        ``check_val_every_n_epoch=None``, which validates\n",
      "                        after every ``N`` training batches across epochs or\n",
      "                        during iteration-based training. Default: ``1.0``.\n",
      "                        (type: Union[int, float, null], default: null)\n",
      "  --trainer.check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH\n",
      "                        Perform a validation loop every after every `N`\n",
      "                        training epochs. If ``None``, validation will be done\n",
      "                        solely based on the number of training batches,\n",
      "                        requiring ``val_check_interval`` to be an integer\n",
      "                        value. Default: ``1``. (type: Optional[int], default:\n",
      "                        1)\n",
      "  --trainer.num_sanity_val_steps NUM_SANITY_VAL_STEPS\n",
      "                        Sanity check runs n validation batches before starting\n",
      "                        the training routine. Set it to `-1` to run all\n",
      "                        batches in all validation dataloaders. Default: ``2``.\n",
      "                        (type: Optional[int], default: null)\n",
      "  --trainer.log_every_n_steps LOG_EVERY_N_STEPS\n",
      "                        How often to log within steps. Default: ``50``. (type:\n",
      "                        Optional[int], default: null)\n",
      "  --trainer.enable_checkpointing {true,false,null}\n",
      "                        If ``True``, enable checkpointing. It will configure a\n",
      "                        default ModelCheckpoint callback if there is no user-\n",
      "                        defined ModelCheckpoint in :paramref:`~lightning.pytor\n",
      "                        ch.trainer.trainer.Trainer.callbacks`. Default:\n",
      "                        ``True``. (type: Optional[bool], default: null)\n",
      "  --trainer.enable_progress_bar {true,false,null}\n",
      "                        Whether to enable to progress bar by default. Default:\n",
      "                        ``True``. (type: Optional[bool], default: null)\n",
      "  --trainer.enable_model_summary {true,false,null}\n",
      "                        Whether to enable model summarization by default.\n",
      "                        Default: ``True``. (type: Optional[bool], default:\n",
      "                        null)\n",
      "  --trainer.accumulate_grad_batches ACCUMULATE_GRAD_BATCHES\n",
      "                        Accumulates gradients over k batches before stepping\n",
      "                        the optimizer. Default: 1. (type: int, default: 1)\n",
      "  --trainer.gradient_clip_val GRADIENT_CLIP_VAL\n",
      "                        The value at which to clip gradients. Passing\n",
      "                        ``gradient_clip_val=None`` disables gradient clipping.\n",
      "                        If using Automatic Mixed Precision (AMP), the\n",
      "                        gradients will be unscaled before. Default: ``None``.\n",
      "                        (type: Union[int, float, null], default: null)\n",
      "  --trainer.gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM\n",
      "                        The gradient clipping algorithm to use. Pass\n",
      "                        ``gradient_clip_algorithm=\"value\"`` to clip by value,\n",
      "                        and ``gradient_clip_algorithm=\"norm\"`` to clip by\n",
      "                        norm. By default it will be set to ``\"norm\"``. (type:\n",
      "                        Optional[str], default: null)\n",
      "  --trainer.deterministic DETERMINISTIC\n",
      "                        If ``True``, sets whether PyTorch operations must use\n",
      "                        deterministic algorithms. Set to ``\"warn\"`` to use\n",
      "                        deterministic algorithms whenever possible, throwing\n",
      "                        warnings on operations that don't support\n",
      "                        deterministic mode. If not set, defaults to ``False``.\n",
      "                        Default: ``None``. (type: Union[bool, Literal['warn'],\n",
      "                        null], default: null)\n",
      "  --trainer.benchmark {true,false,null}\n",
      "                        The value (``True`` or ``False``) to set\n",
      "                        ``torch.backends.cudnn.benchmark`` to. The value for\n",
      "                        ``torch.backends.cudnn.benchmark`` set in the current\n",
      "                        session will be used (``False`` if not manually set).\n",
      "                        If :paramref:`~lightning.pytorch.trainer.trainer.Train\n",
      "                        er.deterministic` is set to ``True``, this will\n",
      "                        default to ``False``. Override to manually set a\n",
      "                        different value. Default: ``None``. (type:\n",
      "                        Optional[bool], default: null)\n",
      "  --trainer.inference_mode {true,false}\n",
      "                        Whether to use :func:`torch.inference_mode` or\n",
      "                        :func:`torch.no_grad` during evaluation\n",
      "                        (``validate``/``test``/``predict``). (type: bool,\n",
      "                        default: True)\n",
      "  --trainer.use_distributed_sampler {true,false}\n",
      "                        Whether to wrap the DataLoader's sampler with\n",
      "                        :class:`torch.utils.data.DistributedSampler`. If not\n",
      "                        specified this is toggled automatically for strategies\n",
      "                        that require it. By default, it will add\n",
      "                        ``shuffle=True`` for the train sampler and\n",
      "                        ``shuffle=False`` for validation/test/predict\n",
      "                        samplers. If you want to disable this logic, you can\n",
      "                        pass ``False`` and add your own distributed sampler in\n",
      "                        the dataloader hooks. If ``True`` and a distributed\n",
      "                        sampler was already added, Lightning will not replace\n",
      "                        the existing one. For iterable-style datasets, we\n",
      "                        don't do this automatically. (type: bool, default:\n",
      "                        True)\n",
      "  --trainer.profiler.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of Profiler and\n",
      "                        exit.\n",
      "  --trainer.profiler PROFILER\n",
      "                        To profile individual steps during training and assist\n",
      "                        in identifying bottlenecks. Default: ``None``. (type:\n",
      "                        Union[Profiler, str, null], default: null, known\n",
      "                        subclasses:\n",
      "                        lightning.pytorch.profilers.AdvancedProfiler,\n",
      "                        lightning.pytorch.profilers.PassThroughProfiler,\n",
      "                        lightning.pytorch.profilers.PyTorchProfiler,\n",
      "                        lightning.pytorch.profilers.SimpleProfiler,\n",
      "                        lightning.pytorch.profilers.XLAProfiler)\n",
      "  --trainer.detect_anomaly {true,false}\n",
      "                        Enable anomaly detection for the autograd engine.\n",
      "                        Default: ``False``. (type: bool, default: False)\n",
      "  --trainer.barebones {true,false}\n",
      "                        Whether to run in \"barebones mode\", where all features\n",
      "                        that may impact raw speed are disabled. This is meant\n",
      "                        for analyzing the Trainer overhead and is discouraged\n",
      "                        during regular training runs. The following features\n",
      "                        are deactivated: :paramref:`~lightning.pytorch.trainer\n",
      "                        .trainer.Trainer.enable_checkpointing`, :paramref:`~li\n",
      "                        ghtning.pytorch.trainer.trainer.Trainer.logger`, :para\n",
      "                        mref:`~lightning.pytorch.trainer.trainer.Trainer.enabl\n",
      "                        e_progress_bar`, :paramref:`~lightning.pytorch.trainer\n",
      "                        .trainer.Trainer.log_every_n_steps`, :paramref:`~light\n",
      "                        ning.pytorch.trainer.trainer.Trainer.enable_model_summ\n",
      "                        ary`, :paramref:`~lightning.pytorch.trainer.trainer.Tr\n",
      "                        ainer.num_sanity_val_steps`, :paramref:`~lightning.pyt\n",
      "                        orch.trainer.trainer.Trainer.fast_dev_run`, :paramref:\n",
      "                        `~lightning.pytorch.trainer.trainer.Trainer.detect_ano\n",
      "                        maly`, :paramref:`~lightning.pytorch.trainer.trainer.T\n",
      "                        rainer.profiler`,\n",
      "                        :meth:`~lightning.pytorch.core.LightningModule.log`, :\n",
      "                        meth:`~lightning.pytorch.core.LightningModule.log_dict\n",
      "                        `. (type: bool, default: False)\n",
      "  --trainer.plugins.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of\n",
      "                        {Precision,ClusterEnvironment,CheckpointIO,LayerSync}\n",
      "                        and exit.\n",
      "  --trainer.plugins PLUGINS, --trainer.plugins+ PLUGINS\n",
      "                        Plugins allow modification of core behavior like ddp\n",
      "                        and amp, and enable custom lightning plugins. Default:\n",
      "                        ``None``. (type: Union[Precision, ClusterEnvironment,\n",
      "                        CheckpointIO, LayerSync, List[Union[Precision,\n",
      "                        ClusterEnvironment, CheckpointIO, LayerSync]], null],\n",
      "                        default: null, known subclasses:\n",
      "                        lightning.pytorch.plugins.Precision,\n",
      "                        lightning.pytorch.plugins.MixedPrecision,\n",
      "                        lightning.pytorch.plugins.BitsandbytesPrecision,\n",
      "                        lightning.pytorch.plugins.DeepSpeedPrecision,\n",
      "                        lightning.pytorch.plugins.DoublePrecision,\n",
      "                        lightning.pytorch.plugins.FSDPPrecision,\n",
      "                        lightning.pytorch.plugins.HalfPrecision,\n",
      "                        lightning.pytorch.plugins.TransformerEnginePrecision,\n",
      "                        lightning.pytorch.plugins.XLAPrecision, lightning.fabr\n",
      "                        ic.plugins.environments.KubeflowEnvironment, lightning\n",
      "                        .fabric.plugins.environments.LightningEnvironment,\n",
      "                        lightning.fabric.plugins.environments.LSFEnvironment,\n",
      "                        lightning.fabric.plugins.environments.MPIEnvironment, \n",
      "                        lightning.fabric.plugins.environments.SLURMEnvironment\n",
      "                        , lightning.fabric.plugins.environments.TorchElasticEn\n",
      "                        vironment,\n",
      "                        lightning.fabric.plugins.environments.XLAEnvironment,\n",
      "                        lightning.fabric.plugins.TorchCheckpointIO,\n",
      "                        lightning.fabric.plugins.XLACheckpointIO,\n",
      "                        lightning.pytorch.plugins.AsyncCheckpointIO,\n",
      "                        lightning.pytorch.plugins.TorchSyncBatchNorm)\n",
      "  --trainer.sync_batchnorm {true,false}\n",
      "                        Synchronize batch norm layers between process\n",
      "                        groups/whole world. Default: ``False``. (type: bool,\n",
      "                        default: False)\n",
      "  --trainer.reload_dataloaders_every_n_epochs RELOAD_DATALOADERS_EVERY_N_EPOCHS\n",
      "                        Set to a positive integer to reload dataloaders every\n",
      "                        n epochs. Default: ``0``. (type: int, default: 0)\n",
      "  --trainer.default_root_dir DEFAULT_ROOT_DIR\n",
      "                        Default path for logs and weights when no\n",
      "                        logger/ckpt_callback passed. Default: ``os.getcwd()``.\n",
      "                        Can be remote file paths such as `s3://mybucket/path`\n",
      "                        or 'hdfs://path/' (type: Union[str, Path, null],\n",
      "                        default: null)\n",
      "\n",
      "<class 'src.model.MultipleChoicesModel'>:\n",
      "  --model CONFIG        Path to a configuration file.\n",
      "  --model.encoder_name ENCODER_NAME\n",
      "                        encoder name; for T5 model, only the encoder will be\n",
      "                        used. (type: Any, default: google/flan-t5-large)\n",
      "  --model.gradient_checkpointing GRADIENT_CHECKPOINTING\n",
      "                        enable gradient checkpointing. (type: Any, default:\n",
      "                        False)\n",
      "  --model.lr LR         learning rate. (type: Any, default: 1e-05)\n",
      "  --model.use_last_hidden_state USE_LAST_HIDDEN_STATE\n",
      "                        use the last hidden state of the encoder's ouput since\n",
      "                        flanT5 didnt add any special token to the start of the\n",
      "                        input. (type: Any, default: True)\n",
      "\n",
      "<class 'src.data.SemevalDataModule'>:\n",
      "  --data CONFIG         Path to a configuration file.\n",
      "  --data.train_data_path TRAIN_DATA_PATH\n",
      "                        (type: Optional[str], default: null)\n",
      "  --data.encoder_name ENCODER_NAME\n",
      "                        (type: str, default: google/flan-t5-large)\n",
      "  --data.promt_style {true,false}\n",
      "                        (type: bool, default: True)\n",
      "  --data.shuffle_choices {true,false}\n",
      "                        (type: bool, default: True)\n",
      "  --data.num_workers NUM_WORKERS\n",
      "                        (type: int, default: 8)\n",
      "  --data.train_batch_size TRAIN_BATCH_SIZE\n",
      "                        (type: int, default: 2)\n",
      "  --data.valid_batch_size VALID_BATCH_SIZE\n",
      "                        (type: int, default: 4)\n",
      "  --data.debug {true,false}\n",
      "                        (type: bool, default: False)\n",
      "\n",
      "Base class for all optimizers:\n",
      "  --optimizer.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of Optimizer and\n",
      "                        exit.\n",
      "  --optimizer CONFIG | CLASS_PATH_OR_NAME | .INIT_ARG_NAME VALUE\n",
      "                        One or more arguments specifying \"class_path\" and\n",
      "                        \"init_args\" for any subclass of torch.optim.Optimizer.\n",
      "                        (type: <class 'Optimizer'>, known subclasses:\n",
      "                        torch.optim.Optimizer, torch.optim.Adadelta,\n",
      "                        torch.optim.Adagrad, torch.optim.Adam,\n",
      "                        torch.optim.AdamW, torch.optim.Adamax,\n",
      "                        torch.optim.ASGD, torch.optim.NAdam,\n",
      "                        torch.optim.RAdam, torch.optim.RMSprop,\n",
      "                        torch.optim.Rprop, torch.optim.SGD,\n",
      "                        torch.optim.SparseAdam, torch.optim.LBFGS,\n",
      "                        transformers.AdamW, transformers.Adafactor)\n",
      "\n",
      "(<class 'torch.optim.lr_scheduler.LRScheduler'>, <class 'lightning.pytorch.cli.ReduceLROnPlateau'>):\n",
      "  --lr_scheduler.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of\n",
      "                        {LRScheduler,ReduceLROnPlateau} and exit.\n",
      "  --lr_scheduler CONFIG | CLASS_PATH_OR_NAME | .INIT_ARG_NAME VALUE\n",
      "                        One or more arguments specifying \"class_path\" and\n",
      "                        \"init_args\" for any subclass of {torch.optim.lr_schedu\n",
      "                        ler.LRScheduler,lightning.pytorch.cli.ReduceLROnPlatea\n",
      "                        u}. (type: Union[LRScheduler, ReduceLROnPlateau],\n",
      "                        known subclasses:\n",
      "                        torch.optim.lr_scheduler.LRScheduler,\n",
      "                        torch.optim.lr_scheduler.LambdaLR,\n",
      "                        transformers.optimization.AdafactorSchedule,\n",
      "                        torch.optim.lr_scheduler.MultiplicativeLR,\n",
      "                        torch.optim.lr_scheduler.StepLR,\n",
      "                        torch.optim.lr_scheduler.MultiStepLR,\n",
      "                        torch.optim.lr_scheduler.ConstantLR,\n",
      "                        torch.optim.lr_scheduler.LinearLR,\n",
      "                        torch.optim.lr_scheduler.ExponentialLR,\n",
      "                        torch.optim.lr_scheduler.SequentialLR,\n",
      "                        torch.optim.lr_scheduler.PolynomialLR,\n",
      "                        torch.optim.lr_scheduler.CosineAnnealingLR,\n",
      "                        torch.optim.lr_scheduler.ChainedScheduler,\n",
      "                        torch.optim.lr_scheduler.CyclicLR,\n",
      "                        torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,\n",
      "                        torch.optim.lr_scheduler.OneCycleLR,\n",
      "                        torch.optim.swa_utils.SWALR,\n",
      "                        lightning.pytorch.cli.ReduceLROnPlateau)\n",
      "\n",
      "Runs the full optimization routine:\n",
      "  --ckpt_path CKPT_PATH\n",
      "                        Path/URL of the checkpoint from which training is\n",
      "                        resumed. Could also be one of two special keywords\n",
      "                        ``\"last\"`` and ``\"hpc\"``. If there is no checkpoint\n",
      "                        file at the path, an exception is raised. (type:\n",
      "                        Optional[str], default: null)\n"
     ]
    }
   ],
   "source": [
    "!python main.py fit --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using the traning script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Seed set to 69\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arch/Projects/college/semeval_brainteaser/src/train.py\", line 108, in <module>\n",
      "    main(args)\n",
      "  File \"/home/arch/Projects/college/semeval_brainteaser/src/train.py\", line 61, in main\n",
      "    model = MultipleChoicesModel(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/Projects/college/semeval_brainteaser/src/model.py\", line 66, in __init__\n",
      "    nn.Dropout(self.encoder.config.hidden_dropout_prob),\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/transformers/configuration_utils.py\", line 265, in __getattribute__\n",
      "    return super().__getattribute__(key)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'T5Config' object has no attribute 'hidden_dropout_prob'\n"
     ]
    }
   ],
   "source": [
    "!python src/train.py \\\n",
    "    --train_data_path dataset/WP-train.npy \\\n",
    "    --eval_data_path dataset/WP_eval_data_for_practice.npy \\\n",
    "    --accelerator cpu \\\n",
    "    --devices 1 \\\n",
    "    --use_last_hidden_state \\\n",
    "    --promt_style \\\n",
    "    --shuffle_choices \\\n",
    "    --train_batch_size 2 \\\n",
    "    --valid_batch_size 4 \\\n",
    "    --max_epochs 1 \\\n",
    "    --debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir dataset\n",
    "\n",
    "!gdown 1VI0EkCUhbyRc2P_i3e0uEUZVr1kxV_jl\n",
    "!gdown 1wCEPQRWB8TSEHKLrAEO1BhiQs6QbqFga\n",
    "\n",
    "!gdown 1nnbhVy_xS4abvf7Ka4ZGCo7TjFSN0R7L\n",
    "!gdown 1K1hsCYuOIfBg0rQgoF1QLK12mXPTC3ow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Seed set to 69\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type              | Params\n",
      "---------------------------------------------------------\n",
      "0 | encoder            | T5EncoderModel    | 341 M \n",
      "1 | choices_classifier | Sequential        | 1.1 M \n",
      "2 | train_f1           | MulticlassF1Score | 0     \n",
      "---------------------------------------------------------\n",
      "342 M     Trainable params\n",
      "0         Non-trainable params\n",
      "342 M     Total params\n",
      "1,369.127 Total estimated model params size (MB)\n",
      "Epoch 0:   0%|                                          | 0/396 [00:00<?, ?it/s]/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/allennlp_light/nn/util.py:1521: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  return torch.cuda.LongTensor(size, device=device).fill_(1).cumsum(0) - 1\n",
      "tensor(0.3467, device='cuda:0')\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arch/Projects/college/semeval_brainteaser/src/train.py\", line 38, in <module>\n",
      "    main(args)\n",
      "  File \"/home/arch/Projects/college/semeval_brainteaser/src/train.py\", line 26, in main\n",
      "    trainer.fit(model, data_loader)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 544, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 44, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 580, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 989, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 1035, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 202, in run\n",
      "    self.advance()\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 359, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 136, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 240, in advance\n",
      "    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 187, in run\n",
      "    self._optimizer_step(batch_idx, closure)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 265, in _optimizer_step\n",
      "    call._call_lightning_module_hook(\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/core/module.py\", line 1291, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py\", line 151, in step\n",
      "    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 230, in optimizer_step\n",
      "    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py\", line 117, in optimizer_step\n",
      "    return optimizer.step(closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 373, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/optim/adamw.py\", line 173, in step\n",
      "    self._init_group(\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/optim/adamw.py\", line 121, in _init_group\n",
      "    state[\"exp_avg\"] = torch.zeros_like(\n",
      "                       ^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 3.81 GiB of which 4.88 MiB is free. Process 9867 has 50.00 MiB memory in use. Including non-PyTorch memory, this process has 3.75 GiB memory in use. Of the allocated memory 3.66 GiB is allocated by PyTorch, and 15.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Epoch 0:   0%|          | 0/396 [00:02<?, ?it/s]                                \n"
     ]
    }
   ],
   "source": [
    "!python src/train.py \\\n",
    "    --accelerator gpu \\\n",
    "    --devices 1 \\\n",
    "    --precision 16 \\\n",
    "    --accumulate_grad_batches 4 \\\n",
    "    --max_epochs 25 \\\n",
    "    --encoder_name \"google/flan-t5-large\" \\\n",
    "    --lr 1e-5 \\\n",
    "    --use_last_hidden_state \\\n",
    "    --promt_style \\\n",
    "    --shuffle_choices \\\n",
    "    --train_data_path \"dataset/WP-train.npy\" \\ \n",
    "    --eval_data_path \"dataset/WP_eval_data_for_practice.npy\" \\\n",
    "    --train_batch_size 2 \\\n",
    "    --valid_batch_size 4 \\\n",
    "    # --wandb \\\n",
    "    # --loss_threshold 0.05 \\\n",
    "    # --no_hidden_layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline \n",
    "!python src/train.py \\\n",
    "    --accelerator gpu \\\n",
    "    --devices 1 \\\n",
    "    --accumulate_grad_batches 1 \\\n",
    "    --max_epochs 20 \\\n",
    "    --encoder_name \"google/flan-t5-xl\" \\\n",
    "    --lr 6e-6 \\\n",
    "    --use_last_hidden_state \\\n",
    "    --promt_style \\\n",
    "    --shuffle_choices \\\n",
    "    --train_data_path \"dataset/WP-train.npy\" \\\n",
    "    --eval_data_path \"dataset/WP_new_test.npy\" \\\n",
    "    --train_batch_size 1 \\\n",
    "    --valid_batch_size 2 \\\n",
    "    --num_workers 6 \\\n",
    "    --reduce_choices \\\n",
    "    --lr_scheduler_gamma 0.85 \\\n",
    "    --wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline \n",
    "!python src/train.py \\\n",
    "    --accelerator gpu \\\n",
    "    --devices 1 \\\n",
    "    --accumulate_grad_batches 1 \\\n",
    "    --max_epochs 20 \\\n",
    "    --encoder_name \"google/flan-t5-xl\" \\\n",
    "    --lr 6e-6 \\\n",
    "    --use_last_hidden_state \\\n",
    "    --promt_style \\\n",
    "    --shuffle_choices \\\n",
    "    --train_data_path \"dataset/WP-train.npy\" \\\n",
    "    --eval_data_path \"dataset/WP_new_test.npy\" \\\n",
    "    --train_batch_size 1 \\\n",
    "    --valid_batch_size 2 \\\n",
    "    --num_workers 6 \\\n",
    "    --reduce_choices \\\n",
    "    --lr_scheduler_gamma 0.85 \\\n",
    "    --wandb \\\n",
    "    --loss_threshold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
