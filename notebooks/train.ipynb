{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arch/Projects/college/semeval_brainteaser\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Seed set to 69\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\n",
      "  | Name               | Type              | Params\n",
      "---------------------------------------------------------\n",
      "0 | encoder            | T5EncoderModel    | 341 M \n",
      "1 | choices_classifier | Sequential        | 1.1 M \n",
      "2 | train_f1           | MulticlassF1Score | 0     \n",
      "---------------------------------------------------------\n",
      "342 M     Trainable params\n",
      "0         Non-trainable params\n",
      "342 M     Total params\n",
      "1,369.127 Total estimated model params size (MB)\n",
      "Epoch 0:   0%|                                          | 0/198 [00:00<?, ?it/s]/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "tensor(1.3882)\n",
      "^C\n",
      "/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "!python src/train.py \\\n",
    "    --data_path dataset/WP-train.npy \\\n",
    "    --accelerator cpu \\\n",
    "    --batch_size 2 \\\n",
    "    # --gradient_checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Seed set to 69\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type              | Params\n",
      "---------------------------------------------------------\n",
      "0 | encoder            | T5EncoderModel    | 341 M \n",
      "1 | choices_classifier | Sequential        | 1.1 M \n",
      "2 | train_f1           | MulticlassF1Score | 0     \n",
      "---------------------------------------------------------\n",
      "342 M     Trainable params\n",
      "0         Non-trainable params\n",
      "342 M     Total params\n",
      "1,369.127 Total estimated model params size (MB)\n",
      "Epoch 0:   0%|                                          | 0/396 [00:00<?, ?it/s]/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/allennlp_light/nn/util.py:1521: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  return torch.cuda.LongTensor(size, device=device).fill_(1).cumsum(0) - 1\n",
      "tensor(0.3467, device='cuda:0')\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arch/Projects/college/semeval_brainteaser/src/train.py\", line 38, in <module>\n",
      "    main(args)\n",
      "  File \"/home/arch/Projects/college/semeval_brainteaser/src/train.py\", line 26, in main\n",
      "    trainer.fit(model, data_loader)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 544, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 44, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 580, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 989, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 1035, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 202, in run\n",
      "    self.advance()\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 359, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 136, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 240, in advance\n",
      "    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 187, in run\n",
      "    self._optimizer_step(batch_idx, closure)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 265, in _optimizer_step\n",
      "    call._call_lightning_module_hook(\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/core/module.py\", line 1291, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py\", line 151, in step\n",
      "    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 230, in optimizer_step\n",
      "    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py\", line 117, in optimizer_step\n",
      "    return optimizer.step(closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 373, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/optim/adamw.py\", line 173, in step\n",
      "    self._init_group(\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/optim/adamw.py\", line 121, in _init_group\n",
      "    state[\"exp_avg\"] = torch.zeros_like(\n",
      "                       ^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 3.81 GiB of which 4.88 MiB is free. Process 9867 has 50.00 MiB memory in use. Including non-PyTorch memory, this process has 3.75 GiB memory in use. Of the allocated memory 3.66 GiB is allocated by PyTorch, and 15.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Epoch 0:   0%|          | 0/396 [00:02<?, ?it/s]                                \n"
     ]
    }
   ],
   "source": [
    "!python src/train.py \\\n",
    "    --data_path dataset/WP-train.npy \\\n",
    "    --accelerator gpu \\\n",
    "    --devices 1 \\\n",
    "    --batch_size 1 \\\n",
    "    --gradient_checkpointing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
