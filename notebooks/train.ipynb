{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arch/Projects/college/semeval_brainteaser\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "usage: main.py [options] fit [-h] [-c CONFIG] [--print_config[=flags]]\n",
      "                             [--seed_everything SEED_EVERYTHING]\n",
      "                             [--trainer CONFIG]\n",
      "                             [--trainer.accelerator.help CLASS_PATH_OR_NAME]\n",
      "                             [--trainer.accelerator ACCELERATOR]\n",
      "                             [--trainer.strategy.help CLASS_PATH_OR_NAME]\n",
      "                             [--trainer.strategy STRATEGY]\n",
      "                             [--trainer.devices DEVICES]\n",
      "                             [--trainer.num_nodes NUM_NODES]\n",
      "                             [--trainer.precision PRECISION]\n",
      "                             [--trainer.logger.help CLASS_PATH_OR_NAME]\n",
      "                             [--trainer.logger LOGGER]\n",
      "                             [--trainer.callbacks.help CLASS_PATH_OR_NAME]\n",
      "                             [--trainer.callbacks CALLBACKS]\n",
      "                             [--trainer.fast_dev_run FAST_DEV_RUN]\n",
      "                             [--trainer.max_epochs MAX_EPOCHS]\n",
      "                             [--trainer.min_epochs MIN_EPOCHS]\n",
      "                             [--trainer.max_steps MAX_STEPS]\n",
      "                             [--trainer.min_steps MIN_STEPS]\n",
      "                             [--trainer.max_time MAX_TIME]\n",
      "                             [--trainer.limit_train_batches LIMIT_TRAIN_BATCHES]\n",
      "                             [--trainer.limit_val_batches LIMIT_VAL_BATCHES]\n",
      "                             [--trainer.limit_test_batches LIMIT_TEST_BATCHES]\n",
      "                             [--trainer.limit_predict_batches LIMIT_PREDICT_BATCHES]\n",
      "                             [--trainer.overfit_batches OVERFIT_BATCHES]\n",
      "                             [--trainer.val_check_interval VAL_CHECK_INTERVAL]\n",
      "                             [--trainer.check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH]\n",
      "                             [--trainer.num_sanity_val_steps NUM_SANITY_VAL_STEPS]\n",
      "                             [--trainer.log_every_n_steps LOG_EVERY_N_STEPS]\n",
      "                             [--trainer.enable_checkpointing {true,false,null}]\n",
      "                             [--trainer.enable_progress_bar {true,false,null}]\n",
      "                             [--trainer.enable_model_summary {true,false,null}]\n",
      "                             [--trainer.accumulate_grad_batches ACCUMULATE_GRAD_BATCHES]\n",
      "                             [--trainer.gradient_clip_val GRADIENT_CLIP_VAL]\n",
      "                             [--trainer.gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM]\n",
      "                             [--trainer.deterministic DETERMINISTIC]\n",
      "                             [--trainer.benchmark {true,false,null}]\n",
      "                             [--trainer.inference_mode {true,false}]\n",
      "                             [--trainer.use_distributed_sampler {true,false}]\n",
      "                             [--trainer.profiler.help CLASS_PATH_OR_NAME]\n",
      "                             [--trainer.profiler PROFILER]\n",
      "                             [--trainer.detect_anomaly {true,false}]\n",
      "                             [--trainer.barebones {true,false}]\n",
      "                             [--trainer.plugins.help CLASS_PATH_OR_NAME]\n",
      "                             [--trainer.plugins PLUGINS]\n",
      "                             [--trainer.sync_batchnorm {true,false}]\n",
      "                             [--trainer.reload_dataloaders_every_n_epochs RELOAD_DATALOADERS_EVERY_N_EPOCHS]\n",
      "                             [--trainer.default_root_dir DEFAULT_ROOT_DIR]\n",
      "                             [--model CONFIG]\n",
      "                             [--model.encoder_name ENCODER_NAME]\n",
      "                             [--model.gradient_checkpointing GRADIENT_CHECKPOINTING]\n",
      "                             [--model.lr LR]\n",
      "                             [--model.use_last_hidden_state USE_LAST_HIDDEN_STATE]\n",
      "                             [--data CONFIG]\n",
      "                             [--data.train_data_path TRAIN_DATA_PATH]\n",
      "                             [--data.encoder_name ENCODER_NAME]\n",
      "                             [--data.promt_style {true,false}]\n",
      "                             [--data.shuffle_choices {true,false}]\n",
      "                             [--data.num_workers NUM_WORKERS]\n",
      "                             [--data.train_batch_size TRAIN_BATCH_SIZE]\n",
      "                             [--data.valid_batch_size VALID_BATCH_SIZE]\n",
      "                             [--data.debug {true,false}]\n",
      "                             [--optimizer.help CLASS_PATH_OR_NAME]\n",
      "                             [--optimizer CONFIG | CLASS_PATH_OR_NAME | .INIT_ARG_NAME VALUE]\n",
      "                             [--lr_scheduler.help CLASS_PATH_OR_NAME]\n",
      "                             [--lr_scheduler CONFIG | CLASS_PATH_OR_NAME | .INIT_ARG_NAME VALUE]\n",
      "                             [--ckpt_path CKPT_PATH]\n",
      "\n",
      "Runs the full optimization routine.\n",
      "\n",
      "options:\n",
      "  -h, --help            Show this help message and exit.\n",
      "  -c CONFIG, --config CONFIG\n",
      "                        Path to a configuration file in json or yaml format.\n",
      "  --print_config[=flags]\n",
      "                        Print the configuration after applying all other\n",
      "                        arguments and exit. The optional flags customizes the\n",
      "                        output and are one or more keywords separated by\n",
      "                        comma. The supported flags are: comments,\n",
      "                        skip_default, skip_null.\n",
      "  --seed_everything SEED_EVERYTHING\n",
      "                        Set to an int to run seed_everything with this value\n",
      "                        before classes instantiation.Set to True to use a\n",
      "                        random seed. (type: Union[bool, int], default: True)\n",
      "\n",
      "Customize every aspect of training via flags:\n",
      "  --trainer CONFIG      Path to a configuration file.\n",
      "  --trainer.accelerator.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of Accelerator\n",
      "                        and exit.\n",
      "  --trainer.accelerator ACCELERATOR\n",
      "                        Supports passing different accelerator types (\"cpu\",\n",
      "                        \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"mps\", \"auto\") as well as\n",
      "                        custom accelerator instances. (type: Union[str,\n",
      "                        Accelerator], default: auto, known subclasses:\n",
      "                        lightning.pytorch.accelerators.CPUAccelerator,\n",
      "                        lightning.pytorch.accelerators.CUDAAccelerator,\n",
      "                        lightning.pytorch.accelerators.MPSAccelerator,\n",
      "                        lightning.pytorch.accelerators.XLAAccelerator)\n",
      "  --trainer.strategy.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of Strategy and\n",
      "                        exit.\n",
      "  --trainer.strategy STRATEGY\n",
      "                        Supports different training strategies with aliases as\n",
      "                        well custom strategies. Default: ``\"auto\"``. (type:\n",
      "                        Union[str, Strategy], default: auto, known subclasses:\n",
      "                        lightning.pytorch.strategies.DDPStrategy,\n",
      "                        lightning.pytorch.strategies.DeepSpeedStrategy,\n",
      "                        lightning.pytorch.strategies.XLAStrategy,\n",
      "                        lightning.pytorch.strategies.FSDPStrategy,\n",
      "                        lightning.pytorch.strategies.SingleDeviceStrategy,\n",
      "                        lightning.pytorch.strategies.SingleDeviceXLAStrategy)\n",
      "  --trainer.devices DEVICES, --trainer.devices+ DEVICES\n",
      "                        The devices to use. Can be set to a positive number\n",
      "                        (int or str), a sequence of device indices (list or\n",
      "                        str), the value ``-1`` to indicate all available\n",
      "                        devices should be used, or ``\"auto\"`` for automatic\n",
      "                        selection based on the chosen accelerator. Default:\n",
      "                        ``\"auto\"``. (type: Union[List[int], str, int],\n",
      "                        default: auto)\n",
      "  --trainer.num_nodes NUM_NODES\n",
      "                        Number of GPU nodes for distributed training. Default:\n",
      "                        ``1``. (type: int, default: 1)\n",
      "  --trainer.precision PRECISION\n",
      "                        Double precision (64, '64' or '64-true'), full\n",
      "                        precision (32, '32' or '32-true'), 16bit mixed\n",
      "                        precision (16, '16', '16-mixed') or bfloat16 mixed\n",
      "                        precision ('bf16', 'bf16-mixed'). Can be used on CPU,\n",
      "                        GPU, TPUs, HPUs or IPUs. Default: ``'32-true'``.\n",
      "                        (type: Union[Literal[64, 32, 16],\n",
      "                        Literal['transformer-engine', 'transformer-engine-\n",
      "                        float16', '16-true', '16-mixed', 'bf16-true',\n",
      "                        'bf16-mixed', '32-true', '64-true'], Literal['64',\n",
      "                        '32', '16', 'bf16'], null], default: null)\n",
      "  --trainer.logger.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of Logger and\n",
      "                        exit.\n",
      "  --trainer.logger LOGGER, --trainer.logger+ LOGGER\n",
      "                        Logger (or iterable collection of loggers) for\n",
      "                        experiment tracking. A ``True`` value uses the default\n",
      "                        ``TensorBoardLogger`` if it is installed, otherwise\n",
      "                        ``CSVLogger``. ``False`` will disable logging. If\n",
      "                        multiple loggers are provided, local files\n",
      "                        (checkpoints, profiler traces, etc.) are saved in the\n",
      "                        ``log_dir`` of the first logger. Default: ``True``.\n",
      "                        (type: Union[Logger, Iterable[Logger], bool, null],\n",
      "                        default: null, known subclasses:\n",
      "                        lightning.pytorch.loggers.logger.DummyLogger,\n",
      "                        lightning.pytorch.loggers.CometLogger,\n",
      "                        lightning.pytorch.loggers.CSVLogger,\n",
      "                        lightning.pytorch.loggers.MLFlowLogger,\n",
      "                        lightning.pytorch.loggers.NeptuneLogger,\n",
      "                        lightning.pytorch.loggers.TensorBoardLogger,\n",
      "                        lightning.pytorch.loggers.WandbLogger)\n",
      "  --trainer.callbacks.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of Callback and\n",
      "                        exit.\n",
      "  --trainer.callbacks CALLBACKS, --trainer.callbacks+ CALLBACKS\n",
      "                        Add a callback or list of callbacks. Default:\n",
      "                        ``None``. (type: Union[List[Callback], Callback,\n",
      "                        null], default: null, known subclasses:\n",
      "                        lightning.Callback,\n",
      "                        lightning.pytorch.callbacks.BatchSizeFinder,\n",
      "                        lightning.pytorch.callbacks.Checkpoint,\n",
      "                        lightning.pytorch.callbacks.ModelCheckpoint,\n",
      "                        lightning.pytorch.callbacks.OnExceptionCheckpoint,\n",
      "                        lightning.pytorch.callbacks.DeviceStatsMonitor,\n",
      "                        lightning.pytorch.callbacks.EarlyStopping,\n",
      "                        lightning.pytorch.callbacks.BaseFinetuning,\n",
      "                        lightning.pytorch.callbacks.BackboneFinetuning, lightn\n",
      "                        ing.pytorch.callbacks.GradientAccumulationScheduler,\n",
      "                        lightning.pytorch.callbacks.LambdaCallback,\n",
      "                        lightning.pytorch.callbacks.LearningRateFinder,\n",
      "                        lightning.pytorch.callbacks.LearningRateMonitor,\n",
      "                        lightning.pytorch.callbacks.ModelSummary,\n",
      "                        lightning.pytorch.callbacks.RichModelSummary,\n",
      "                        lightning.pytorch.callbacks.BasePredictionWriter,\n",
      "                        lightning.pytorch.callbacks.ProgressBar,\n",
      "                        lightning.pytorch.callbacks.RichProgressBar,\n",
      "                        lightning.pytorch.callbacks.TQDMProgressBar,\n",
      "                        lightning.pytorch.callbacks.Timer,\n",
      "                        lightning.pytorch.callbacks.ModelPruning,\n",
      "                        lightning.pytorch.callbacks.SpikeDetection,\n",
      "                        lightning.pytorch.callbacks.StochasticWeightAveraging,\n",
      "                        lightning.pytorch.cli.SaveConfigCallback)\n",
      "  --trainer.fast_dev_run FAST_DEV_RUN\n",
      "                        Runs n if set to ``n`` (int) else 1 if set to ``True``\n",
      "                        batch(es) of train, val and test to find any bugs (ie:\n",
      "                        a sort of unit test). Default: ``False``. (type:\n",
      "                        Union[int, bool], default: False)\n",
      "  --trainer.max_epochs MAX_EPOCHS\n",
      "                        Stop training once this number of epochs is reached.\n",
      "                        Disabled by default (None). If both max_epochs and\n",
      "                        max_steps are not specified, defaults to ``max_epochs\n",
      "                        = 1000``. To enable infinite training, set\n",
      "                        ``max_epochs = -1``. (type: Optional[int], default:\n",
      "                        null)\n",
      "  --trainer.min_epochs MIN_EPOCHS\n",
      "                        Force training for at least these many epochs.\n",
      "                        Disabled by default (None). (type: Optional[int],\n",
      "                        default: null)\n",
      "  --trainer.max_steps MAX_STEPS\n",
      "                        Stop training after this number of steps. Disabled by\n",
      "                        default (-1). If ``max_steps = -1`` and ``max_epochs =\n",
      "                        None``, will default to ``max_epochs = 1000``. To\n",
      "                        enable infinite training, set ``max_epochs`` to\n",
      "                        ``-1``. (type: int, default: -1)\n",
      "  --trainer.min_steps MIN_STEPS\n",
      "                        Force training for at least these number of steps.\n",
      "                        Disabled by default (``None``). (type: Optional[int],\n",
      "                        default: null)\n",
      "  --trainer.max_time MAX_TIME\n",
      "                        Stop training after this amount of time has passed.\n",
      "                        Disabled by default (``None``). The time duration can\n",
      "                        be specified in the format DD:HH:MM:SS (days, hours,\n",
      "                        minutes seconds), as a :class:`datetime.timedelta`, or\n",
      "                        a dictionary with keys that will be passed to\n",
      "                        :class:`datetime.timedelta`. (type: Union[str,\n",
      "                        timedelta, Dict[str, int], null], default: null)\n",
      "  --trainer.limit_train_batches LIMIT_TRAIN_BATCHES\n",
      "                        How much of training dataset to check (float =\n",
      "                        fraction, int = num_batches). Default: ``1.0``. (type:\n",
      "                        Union[int, float, null], default: null)\n",
      "  --trainer.limit_val_batches LIMIT_VAL_BATCHES\n",
      "                        How much of validation dataset to check (float =\n",
      "                        fraction, int = num_batches). Default: ``1.0``. (type:\n",
      "                        Union[int, float, null], default: null)\n",
      "  --trainer.limit_test_batches LIMIT_TEST_BATCHES\n",
      "                        How much of test dataset to check (float = fraction,\n",
      "                        int = num_batches). Default: ``1.0``. (type:\n",
      "                        Union[int, float, null], default: null)\n",
      "  --trainer.limit_predict_batches LIMIT_PREDICT_BATCHES\n",
      "                        How much of prediction dataset to check (float =\n",
      "                        fraction, int = num_batches). Default: ``1.0``. (type:\n",
      "                        Union[int, float, null], default: null)\n",
      "  --trainer.overfit_batches OVERFIT_BATCHES\n",
      "                        Overfit a fraction of training/validation data (float)\n",
      "                        or a set number of batches (int). Default: ``0.0``.\n",
      "                        (type: Union[int, float], default: 0.0)\n",
      "  --trainer.val_check_interval VAL_CHECK_INTERVAL\n",
      "                        How often to check the validation set. Pass a\n",
      "                        ``float`` in the range [0.0, 1.0] to check after a\n",
      "                        fraction of the training epoch. Pass an ``int`` to\n",
      "                        check after a fixed number of training batches. An\n",
      "                        ``int`` value can only be higher than the number of\n",
      "                        training batches when\n",
      "                        ``check_val_every_n_epoch=None``, which validates\n",
      "                        after every ``N`` training batches across epochs or\n",
      "                        during iteration-based training. Default: ``1.0``.\n",
      "                        (type: Union[int, float, null], default: null)\n",
      "  --trainer.check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH\n",
      "                        Perform a validation loop every after every `N`\n",
      "                        training epochs. If ``None``, validation will be done\n",
      "                        solely based on the number of training batches,\n",
      "                        requiring ``val_check_interval`` to be an integer\n",
      "                        value. Default: ``1``. (type: Optional[int], default:\n",
      "                        1)\n",
      "  --trainer.num_sanity_val_steps NUM_SANITY_VAL_STEPS\n",
      "                        Sanity check runs n validation batches before starting\n",
      "                        the training routine. Set it to `-1` to run all\n",
      "                        batches in all validation dataloaders. Default: ``2``.\n",
      "                        (type: Optional[int], default: null)\n",
      "  --trainer.log_every_n_steps LOG_EVERY_N_STEPS\n",
      "                        How often to log within steps. Default: ``50``. (type:\n",
      "                        Optional[int], default: null)\n",
      "  --trainer.enable_checkpointing {true,false,null}\n",
      "                        If ``True``, enable checkpointing. It will configure a\n",
      "                        default ModelCheckpoint callback if there is no user-\n",
      "                        defined ModelCheckpoint in :paramref:`~lightning.pytor\n",
      "                        ch.trainer.trainer.Trainer.callbacks`. Default:\n",
      "                        ``True``. (type: Optional[bool], default: null)\n",
      "  --trainer.enable_progress_bar {true,false,null}\n",
      "                        Whether to enable to progress bar by default. Default:\n",
      "                        ``True``. (type: Optional[bool], default: null)\n",
      "  --trainer.enable_model_summary {true,false,null}\n",
      "                        Whether to enable model summarization by default.\n",
      "                        Default: ``True``. (type: Optional[bool], default:\n",
      "                        null)\n",
      "  --trainer.accumulate_grad_batches ACCUMULATE_GRAD_BATCHES\n",
      "                        Accumulates gradients over k batches before stepping\n",
      "                        the optimizer. Default: 1. (type: int, default: 1)\n",
      "  --trainer.gradient_clip_val GRADIENT_CLIP_VAL\n",
      "                        The value at which to clip gradients. Passing\n",
      "                        ``gradient_clip_val=None`` disables gradient clipping.\n",
      "                        If using Automatic Mixed Precision (AMP), the\n",
      "                        gradients will be unscaled before. Default: ``None``.\n",
      "                        (type: Union[int, float, null], default: null)\n",
      "  --trainer.gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM\n",
      "                        The gradient clipping algorithm to use. Pass\n",
      "                        ``gradient_clip_algorithm=\"value\"`` to clip by value,\n",
      "                        and ``gradient_clip_algorithm=\"norm\"`` to clip by\n",
      "                        norm. By default it will be set to ``\"norm\"``. (type:\n",
      "                        Optional[str], default: null)\n",
      "  --trainer.deterministic DETERMINISTIC\n",
      "                        If ``True``, sets whether PyTorch operations must use\n",
      "                        deterministic algorithms. Set to ``\"warn\"`` to use\n",
      "                        deterministic algorithms whenever possible, throwing\n",
      "                        warnings on operations that don't support\n",
      "                        deterministic mode. If not set, defaults to ``False``.\n",
      "                        Default: ``None``. (type: Union[bool, Literal['warn'],\n",
      "                        null], default: null)\n",
      "  --trainer.benchmark {true,false,null}\n",
      "                        The value (``True`` or ``False``) to set\n",
      "                        ``torch.backends.cudnn.benchmark`` to. The value for\n",
      "                        ``torch.backends.cudnn.benchmark`` set in the current\n",
      "                        session will be used (``False`` if not manually set).\n",
      "                        If :paramref:`~lightning.pytorch.trainer.trainer.Train\n",
      "                        er.deterministic` is set to ``True``, this will\n",
      "                        default to ``False``. Override to manually set a\n",
      "                        different value. Default: ``None``. (type:\n",
      "                        Optional[bool], default: null)\n",
      "  --trainer.inference_mode {true,false}\n",
      "                        Whether to use :func:`torch.inference_mode` or\n",
      "                        :func:`torch.no_grad` during evaluation\n",
      "                        (``validate``/``test``/``predict``). (type: bool,\n",
      "                        default: True)\n",
      "  --trainer.use_distributed_sampler {true,false}\n",
      "                        Whether to wrap the DataLoader's sampler with\n",
      "                        :class:`torch.utils.data.DistributedSampler`. If not\n",
      "                        specified this is toggled automatically for strategies\n",
      "                        that require it. By default, it will add\n",
      "                        ``shuffle=True`` for the train sampler and\n",
      "                        ``shuffle=False`` for validation/test/predict\n",
      "                        samplers. If you want to disable this logic, you can\n",
      "                        pass ``False`` and add your own distributed sampler in\n",
      "                        the dataloader hooks. If ``True`` and a distributed\n",
      "                        sampler was already added, Lightning will not replace\n",
      "                        the existing one. For iterable-style datasets, we\n",
      "                        don't do this automatically. (type: bool, default:\n",
      "                        True)\n",
      "  --trainer.profiler.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of Profiler and\n",
      "                        exit.\n",
      "  --trainer.profiler PROFILER\n",
      "                        To profile individual steps during training and assist\n",
      "                        in identifying bottlenecks. Default: ``None``. (type:\n",
      "                        Union[Profiler, str, null], default: null, known\n",
      "                        subclasses:\n",
      "                        lightning.pytorch.profilers.AdvancedProfiler,\n",
      "                        lightning.pytorch.profilers.PassThroughProfiler,\n",
      "                        lightning.pytorch.profilers.PyTorchProfiler,\n",
      "                        lightning.pytorch.profilers.SimpleProfiler,\n",
      "                        lightning.pytorch.profilers.XLAProfiler)\n",
      "  --trainer.detect_anomaly {true,false}\n",
      "                        Enable anomaly detection for the autograd engine.\n",
      "                        Default: ``False``. (type: bool, default: False)\n",
      "  --trainer.barebones {true,false}\n",
      "                        Whether to run in \"barebones mode\", where all features\n",
      "                        that may impact raw speed are disabled. This is meant\n",
      "                        for analyzing the Trainer overhead and is discouraged\n",
      "                        during regular training runs. The following features\n",
      "                        are deactivated: :paramref:`~lightning.pytorch.trainer\n",
      "                        .trainer.Trainer.enable_checkpointing`, :paramref:`~li\n",
      "                        ghtning.pytorch.trainer.trainer.Trainer.logger`, :para\n",
      "                        mref:`~lightning.pytorch.trainer.trainer.Trainer.enabl\n",
      "                        e_progress_bar`, :paramref:`~lightning.pytorch.trainer\n",
      "                        .trainer.Trainer.log_every_n_steps`, :paramref:`~light\n",
      "                        ning.pytorch.trainer.trainer.Trainer.enable_model_summ\n",
      "                        ary`, :paramref:`~lightning.pytorch.trainer.trainer.Tr\n",
      "                        ainer.num_sanity_val_steps`, :paramref:`~lightning.pyt\n",
      "                        orch.trainer.trainer.Trainer.fast_dev_run`, :paramref:\n",
      "                        `~lightning.pytorch.trainer.trainer.Trainer.detect_ano\n",
      "                        maly`, :paramref:`~lightning.pytorch.trainer.trainer.T\n",
      "                        rainer.profiler`,\n",
      "                        :meth:`~lightning.pytorch.core.LightningModule.log`, :\n",
      "                        meth:`~lightning.pytorch.core.LightningModule.log_dict\n",
      "                        `. (type: bool, default: False)\n",
      "  --trainer.plugins.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of\n",
      "                        {Precision,ClusterEnvironment,CheckpointIO,LayerSync}\n",
      "                        and exit.\n",
      "  --trainer.plugins PLUGINS, --trainer.plugins+ PLUGINS\n",
      "                        Plugins allow modification of core behavior like ddp\n",
      "                        and amp, and enable custom lightning plugins. Default:\n",
      "                        ``None``. (type: Union[Precision, ClusterEnvironment,\n",
      "                        CheckpointIO, LayerSync, List[Union[Precision,\n",
      "                        ClusterEnvironment, CheckpointIO, LayerSync]], null],\n",
      "                        default: null, known subclasses:\n",
      "                        lightning.pytorch.plugins.Precision,\n",
      "                        lightning.pytorch.plugins.MixedPrecision,\n",
      "                        lightning.pytorch.plugins.BitsandbytesPrecision,\n",
      "                        lightning.pytorch.plugins.DeepSpeedPrecision,\n",
      "                        lightning.pytorch.plugins.DoublePrecision,\n",
      "                        lightning.pytorch.plugins.FSDPPrecision,\n",
      "                        lightning.pytorch.plugins.HalfPrecision,\n",
      "                        lightning.pytorch.plugins.TransformerEnginePrecision,\n",
      "                        lightning.pytorch.plugins.XLAPrecision, lightning.fabr\n",
      "                        ic.plugins.environments.KubeflowEnvironment, lightning\n",
      "                        .fabric.plugins.environments.LightningEnvironment,\n",
      "                        lightning.fabric.plugins.environments.LSFEnvironment,\n",
      "                        lightning.fabric.plugins.environments.MPIEnvironment, \n",
      "                        lightning.fabric.plugins.environments.SLURMEnvironment\n",
      "                        , lightning.fabric.plugins.environments.TorchElasticEn\n",
      "                        vironment,\n",
      "                        lightning.fabric.plugins.environments.XLAEnvironment,\n",
      "                        lightning.fabric.plugins.TorchCheckpointIO,\n",
      "                        lightning.fabric.plugins.XLACheckpointIO,\n",
      "                        lightning.pytorch.plugins.AsyncCheckpointIO,\n",
      "                        lightning.pytorch.plugins.TorchSyncBatchNorm)\n",
      "  --trainer.sync_batchnorm {true,false}\n",
      "                        Synchronize batch norm layers between process\n",
      "                        groups/whole world. Default: ``False``. (type: bool,\n",
      "                        default: False)\n",
      "  --trainer.reload_dataloaders_every_n_epochs RELOAD_DATALOADERS_EVERY_N_EPOCHS\n",
      "                        Set to a positive integer to reload dataloaders every\n",
      "                        n epochs. Default: ``0``. (type: int, default: 0)\n",
      "  --trainer.default_root_dir DEFAULT_ROOT_DIR\n",
      "                        Default path for logs and weights when no\n",
      "                        logger/ckpt_callback passed. Default: ``os.getcwd()``.\n",
      "                        Can be remote file paths such as `s3://mybucket/path`\n",
      "                        or 'hdfs://path/' (type: Union[str, Path, null],\n",
      "                        default: null)\n",
      "\n",
      "<class 'src.model.MultipleChoicesModel'>:\n",
      "  --model CONFIG        Path to a configuration file.\n",
      "  --model.encoder_name ENCODER_NAME\n",
      "                        encoder name; for T5 model, only the encoder will be\n",
      "                        used. (type: Any, default: google/flan-t5-large)\n",
      "  --model.gradient_checkpointing GRADIENT_CHECKPOINTING\n",
      "                        enable gradient checkpointing. (type: Any, default:\n",
      "                        False)\n",
      "  --model.lr LR         learning rate. (type: Any, default: 1e-05)\n",
      "  --model.use_last_hidden_state USE_LAST_HIDDEN_STATE\n",
      "                        use the last hidden state of the encoder's ouput since\n",
      "                        flanT5 didnt add any special token to the start of the\n",
      "                        input. (type: Any, default: True)\n",
      "\n",
      "<class 'src.data.SemevalDataModule'>:\n",
      "  --data CONFIG         Path to a configuration file.\n",
      "  --data.train_data_path TRAIN_DATA_PATH\n",
      "                        (type: Optional[str], default: null)\n",
      "  --data.encoder_name ENCODER_NAME\n",
      "                        (type: str, default: google/flan-t5-large)\n",
      "  --data.promt_style {true,false}\n",
      "                        (type: bool, default: True)\n",
      "  --data.shuffle_choices {true,false}\n",
      "                        (type: bool, default: True)\n",
      "  --data.num_workers NUM_WORKERS\n",
      "                        (type: int, default: 8)\n",
      "  --data.train_batch_size TRAIN_BATCH_SIZE\n",
      "                        (type: int, default: 2)\n",
      "  --data.valid_batch_size VALID_BATCH_SIZE\n",
      "                        (type: int, default: 4)\n",
      "  --data.debug {true,false}\n",
      "                        (type: bool, default: False)\n",
      "\n",
      "Base class for all optimizers:\n",
      "  --optimizer.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of Optimizer and\n",
      "                        exit.\n",
      "  --optimizer CONFIG | CLASS_PATH_OR_NAME | .INIT_ARG_NAME VALUE\n",
      "                        One or more arguments specifying \"class_path\" and\n",
      "                        \"init_args\" for any subclass of torch.optim.Optimizer.\n",
      "                        (type: <class 'Optimizer'>, known subclasses:\n",
      "                        torch.optim.Optimizer, torch.optim.Adadelta,\n",
      "                        torch.optim.Adagrad, torch.optim.Adam,\n",
      "                        torch.optim.AdamW, torch.optim.Adamax,\n",
      "                        torch.optim.ASGD, torch.optim.NAdam,\n",
      "                        torch.optim.RAdam, torch.optim.RMSprop,\n",
      "                        torch.optim.Rprop, torch.optim.SGD,\n",
      "                        torch.optim.SparseAdam, torch.optim.LBFGS,\n",
      "                        transformers.AdamW, transformers.Adafactor)\n",
      "\n",
      "(<class 'torch.optim.lr_scheduler.LRScheduler'>, <class 'lightning.pytorch.cli.ReduceLROnPlateau'>):\n",
      "  --lr_scheduler.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of\n",
      "                        {LRScheduler,ReduceLROnPlateau} and exit.\n",
      "  --lr_scheduler CONFIG | CLASS_PATH_OR_NAME | .INIT_ARG_NAME VALUE\n",
      "                        One or more arguments specifying \"class_path\" and\n",
      "                        \"init_args\" for any subclass of {torch.optim.lr_schedu\n",
      "                        ler.LRScheduler,lightning.pytorch.cli.ReduceLROnPlatea\n",
      "                        u}. (type: Union[LRScheduler, ReduceLROnPlateau],\n",
      "                        known subclasses:\n",
      "                        torch.optim.lr_scheduler.LRScheduler,\n",
      "                        torch.optim.lr_scheduler.LambdaLR,\n",
      "                        transformers.optimization.AdafactorSchedule,\n",
      "                        torch.optim.lr_scheduler.MultiplicativeLR,\n",
      "                        torch.optim.lr_scheduler.StepLR,\n",
      "                        torch.optim.lr_scheduler.MultiStepLR,\n",
      "                        torch.optim.lr_scheduler.ConstantLR,\n",
      "                        torch.optim.lr_scheduler.LinearLR,\n",
      "                        torch.optim.lr_scheduler.ExponentialLR,\n",
      "                        torch.optim.lr_scheduler.SequentialLR,\n",
      "                        torch.optim.lr_scheduler.PolynomialLR,\n",
      "                        torch.optim.lr_scheduler.CosineAnnealingLR,\n",
      "                        torch.optim.lr_scheduler.ChainedScheduler,\n",
      "                        torch.optim.lr_scheduler.CyclicLR,\n",
      "                        torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,\n",
      "                        torch.optim.lr_scheduler.OneCycleLR,\n",
      "                        torch.optim.swa_utils.SWALR,\n",
      "                        lightning.pytorch.cli.ReduceLROnPlateau)\n",
      "\n",
      "Runs the full optimization routine:\n",
      "  --ckpt_path CKPT_PATH\n",
      "                        Path/URL of the checkpoint from which training is\n",
      "                        resumed. Could also be one of two special keywords\n",
      "                        ``\"last\"`` and ``\"hpc\"``. If there is no checkpoint\n",
      "                        file at the path, an exception is raised. (type:\n",
      "                        Optional[str], default: null)\n"
     ]
    }
   ],
   "source": [
    "!python main.py fit --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/fabric/utilities/seed.py:40: No seed found, seed set to 3225013099\n",
      "Seed set to 3225013099\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[rank: 0] Seed set to 3225013099\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type              | Params\n",
      "---------------------------------------------------------\n",
      "0 | encoder            | XLMRobertaModel   | 277 M \n",
      "1 | choices_classifier | Sequential        | 1.2 M \n",
      "2 | train_f1           | MulticlassF1Score | 0     \n",
      "3 | valid_f1           | MulticlassF1Score | 0     \n",
      "---------------------------------------------------------\n",
      "278 M     Trainable params\n",
      "0         Non-trainable params\n",
      "278 M     Total params\n",
      "1,114.537 Total estimated model params size (MB)\n",
      "Sanity Checking DataLoader 0:   0%|                       | 0/2 [00:00<?, ?it/s]/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/allennlp_light/nn/util.py:1521: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  return torch.cuda.LongTensor(size, device=device).fill_(1).cumsum(0) - 1\n",
      "Sanity Checking DataLoader 0: 100%|███████████████| 2/2 [00:00<00:00,  3.98it/s]/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:433: It is recommended to use `self.log('valid_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "terminate called after throwing an instance of 'c10::Error'                     \n",
      "  what():  CUDA error: initialization error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:44 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f3337392617 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f333734d98d in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f3342fcb128 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)\n",
      "frame #3: c10::cuda::ExchangeDevice(int) + 0x8a (0x7f3342fcb58a in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)\n",
      "frame #4: <unknown function> + 0xe24342 (0x7f32c2824342 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #5: <unknown function> + 0x513d26 (0x7f3304913d26 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #6: <unknown function> + 0x55ca7 (0x7f3337377ca7 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #7: c10::TensorImpl::~TensorImpl() + 0x1e3 (0x7f333736fcb3 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #8: c10::TensorImpl::~TensorImpl() + 0x9 (0x7f333736fe49 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #9: <unknown function> + 0x7c8698 (0x7f3304bc8698 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #10: THPVariable_subclass_dealloc(_object*) + 0x305 (0x7f3304bc8a25 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\n",
      "<omitting python frames>\n",
      "\n",
      "Epoch 0:   0%|                                          | 0/456 [00:00<?, ?it/s]terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Errorc10::Error'\n",
      "'\n",
      "  what():    what():  CUDA error: initialization error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:44 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f3337392617 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f333734d98d in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f3342fcb128 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)\n",
      "frame #3: c10::cuda::ExchangeDevice(int) + 0x8a (0x7f3342fcb58a in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)\n",
      "frame #4: <unknown function> + 0xe24342 (0x7f32c2824342 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #5: <unknown function> + 0x513d26 (0x7f3304913d26 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #6: <unknown function> + 0x55ca7 (0x7f3337377ca7 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #7: c10::TensorImpl::~TensorImpl() + 0x1e3 (0x7f333736fcb3 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #8: c10::TensorImpl::~TensorImpl() + 0x9 (0x7f333736fe49 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #9: <unknown function> + 0x7c8698 (0x7f3304bc8698 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #10: THPVariable_subclass_dealloc(_object*) + 0x305 (0x7f3304bc8a25 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\n",
      "<omitting python frames>\n",
      "frame #54: <unknown function> + 0x8c9eb (0x7f3344aaa9eb in /usr/lib/libc.so.6)\n",
      "frame #55: <unknown function> + 0x1107cc (0x7f3344b2e7cc in /usr/lib/libc.so.6)\n",
      "CUDA error: initialization error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:44 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f3337392617 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f333734d98d in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f3342fcb128 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)\n",
      "frame #3: c10::cuda::ExchangeDevice(int) + 0x8a (0x7f3342fcb58a in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)\n",
      "frame #4: <unknown function> + 0xe24342 (0x7f32c2824342 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #5: <unknown function> + 0x513d26 (0x7f3304913d26 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #6: <unknown function> + 0x55ca7 (0x7f3337377ca7 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #7: c10::TensorImpl::~TensorImpl() + 0x1e3 (0x7f333736fcb3 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #8: c10::TensorImpl::~TensorImpl() + 0x9 (0x7f333736fe49 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #9: <unknown function> + 0x7c8698 (0x7f3304bc8698 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #10: THPVariable_subclass_dealloc(_object*) + 0x305 (0x7f3304bc8a25 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\n",
      "<omitting python frames>\n",
      "\n",
      "\n",
      "terminate called after throwing an instance of 'c10::Error'\n",
      "  what():  CUDA error: initialization error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:44 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f3337392617 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f333734d98d in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f3342fcb128 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)\n",
      "frame #3: c10::cuda::ExchangeDevice(int) + 0x8a (0x7f3342fcb58a in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)\n",
      "frame #4: <unknown function> + 0xe24342 (0x7f32c2824342 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #5: <unknown function> + 0x513d26 (0x7f3304913d26 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #6: <unknown function> + 0x55ca7 (0x7f3337377ca7 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #7: c10::TensorImpl::~TensorImpl() + 0x1e3 (0x7f333736fcb3 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #8: c10::TensorImpl::~TensorImpl() + 0x9 (0x7f333736fe49 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #9: <unknown function> + 0x7c8698 (0x7f3304bc8698 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #10: THPVariable_subclass_dealloc(_object*) + 0x305 (0x7f3304bc8a25 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\n",
      "<omitting python frames>\n",
      "frame #52: <unknown function> + 0xaefe (0x7f33437e8efe in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #53: <unknown function> + 0xa576 (0x7f33437e8576 in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #54: <unknown function> + 0x935f (0x7f33437e735f in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #55: <unknown function> + 0xb127 (0x7f33437e9127 in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #56: <unknown function> + 0x9595 (0x7f33437e7595 in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #57: <unknown function> + 0x935f (0x7f33437e735f in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #58: <unknown function> + 0xb127 (0x7f33437e9127 in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #59: <unknown function> + 0x9595 (0x7f33437e7595 in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #60: <unknown function> + 0xa127 (0x7f33437e8127 in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #61: <unknown function> + 0x935f (0x7f33437e735f in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #62: <unknown function> + 0x1217c (0x7f33437f017c in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "\n",
      "terminate called after throwing an instance of 'c10::Error'\n",
      "  what():  CUDA error: initialization error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:44 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f3337392617 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f333734d98d in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f3342fcb128 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)\n",
      "frame #3: c10::cuda::ExchangeDevice(int) + 0x8a (0x7f3342fcb58a in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)\n",
      "frame #4: <unknown function> + 0xe24342 (0x7f32c2824342 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #5: <unknown function> + 0x513d26 (0x7f3304913d26 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #6: <unknown function> + 0x55ca7 (0x7f3337377ca7 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #7: c10::TensorImpl::~TensorImpl() + 0x1e3 (0x7f333736fcb3 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #8: c10::TensorImpl::~TensorImpl() + 0x9 (0x7f333736fe49 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #9: <unknown function> + 0x7c8698 (0x7f3304bc8698 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #10: THPVariable_subclass_dealloc(_object*) + 0x305 (0x7f3304bc8a25 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\n",
      "<omitting python frames>\n",
      "frame #52: <unknown function> + 0xaefe (0x7f33437e8efe in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #53: <unknown function> + 0xa576 (0x7f33437e8576 in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #54: <unknown function> + 0x935f (0x7f33437e735f in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #55: <unknown function> + 0xb127 (0x7f33437e9127 in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #56: <unknown function> + 0x9595 (0x7f33437e7595 in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #57: <unknown function> + 0x935f (0x7f33437e735f in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #58: <unknown function> + 0xb127 (0x7f33437e9127 in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #59: <unknown function> + 0x9595 (0x7f33437e7595 in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #60: <unknown function> + 0xa127 (0x7f33437e8127 in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #61: <unknown function> + 0x935f (0x7f33437e735f in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #62: <unknown function> + 0x1217c (0x7f33437f017c in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "\n",
      "terminate called after throwing an instance of 'c10::Error'\n",
      "  what():  CUDA error: initialization error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:44 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f3337392617 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f333734d98d in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f3342fcb128 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)\n",
      "frame #3: c10::cuda::ExchangeDevice(int) + 0x8a (0x7f3342fcb58a in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)\n",
      "frame #4: <unknown function> + 0xe24342 (0x7f32c2824342 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #5: <unknown function> + 0x513d26 (0x7f3304913d26 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #6: <unknown function> + 0x55ca7 (0x7f3337377ca7 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #7: c10::TensorImpl::~TensorImpl() + 0x1e3 (0x7f333736fcb3 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #8: c10::TensorImpl::~TensorImpl() + 0x9 (0x7f333736fe49 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #9: <unknown function> + 0x7c8698 (0x7f3304bc8698 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #10: THPVariable_subclass_dealloc(_object*) + 0x305 (0x7f3304bc8a25 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\n",
      "<omitting python frames>\n",
      "frame #52: <unknown function> + 0xaefe (0x7f33437e8efe in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #53: <unknown function> + 0xa576 (0x7f33437e8576 in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #54: <unknown function> + 0x935f (0x7f33437e735f in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #55: <unknown function> + 0xb127 (0x7f33437e9127 in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #56: <unknown function> + 0x9595 (0x7f33437e7595 in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #57: <unknown function> + 0x935f (0x7f33437e735f in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #58: <unknown function> + 0xb127 (0x7f33437e9127 in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #59: <unknown function> + 0x9595 (0x7f33437e7595 in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #60: <unknown function> + 0xa127 (0x7f33437e8127 in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #61: <unknown function> + 0x935f (0x7f33437e735f in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "frame #62: <unknown function> + 0x1217c (0x7f33437f017c in /usr/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so)\n",
      "\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f327a287880>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1424, in _shutdown_workers\n",
      "    self._pin_memory_thread.join()\n",
      "  File \"/usr/lib/python3.11/threading.py\", line 1119, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/usr/lib/python3.11/threading.py\", line 1139, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 34411) is killed by signal: Aborted. \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arch/Projects/college/semeval_brainteaser/main.py\", line 12, in <module>\n",
      "    cli_main()\n",
      "  File \"/home/arch/Projects/college/semeval_brainteaser/main.py\", line 9, in cli_main\n",
      "    cli = LightningCLI(MultipleChoicesModel, SemevalDataModule)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 386, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 677, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 544, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 43, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 102, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 580, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 989, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 1035, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 202, in run\n",
      "    self.advance()\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 359, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 136, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 240, in advance\n",
      "    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 187, in run\n",
      "    self._optimizer_step(batch_idx, closure)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 265, in _optimizer_step\n",
      "    call._call_lightning_module_hook(\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/core/module.py\", line 1291, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py\", line 151, in step\n",
      "    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 230, in optimizer_step\n",
      "    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/fsdp.py\", line 145, in optimizer_step\n",
      "    return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py\", line 117, in optimizer_step\n",
      "    return optimizer.step(closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 373, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/optim/adamw.py\", line 173, in step\n",
      "    self._init_group(\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/optim/adamw.py\", line 121, in _init_group\n",
      "    state[\"exp_avg\"] = torch.zeros_like(\n",
      "                       ^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 3.81 GiB of which 8.88 MiB is free. Including non-PyTorch memory, this process has 3.80 GiB memory in use. Of the allocated memory 3.56 GiB is allocated by PyTorch, and 15.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Exception in thread Thread-3 (_pin_memory_loop):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py\", line 54, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py\", line 31, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/multiprocessing/reductions.py\", line 355, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 524, in Client\n",
      "    answer_challenge(c, authkey)\n",
      "  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 768, in answer_challenge\n",
      "    message = connection.recv_bytes(256)         # reject large message\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    }
   ],
   "source": [
    "!python main.py fit \\\n",
    "    --trainer.accelerator \"gpu\" \\\n",
    "    --data.train_data_path \"dataset/SP-train.npy\" \\\n",
    "    --model.gradient_checkpointing True \\\n",
    "    --trainer.max_epochs 1 \\\n",
    "    --model.encoder_name \"xlm-roberta-base\" \\\n",
    "    --data.encoder_name \"xlm-roberta-base\" \\\n",
    "    --data.promt_style False \\\n",
    "    --model.use_last_hidden_state False \\\n",
    "    --data.train_batch_size 1 \\\n",
    "    --data.valid_batch_size 1 \\\n",
    "    --trainer.strategy \"fsdp\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/fabric/utilities/seed.py:40: No seed found, seed set to 4001728581\n",
      "Seed set to 4001728581\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name               | Type              | Params\n",
      "---------------------------------------------------------\n",
      "0 | encoder            | T5EncoderModel    | 341 M \n",
      "1 | choices_classifier | Sequential        | 2.1 M \n",
      "2 | train_f1           | MulticlassF1Score | 0     \n",
      "3 | valid_f1           | MulticlassF1Score | 0     \n",
      "---------------------------------------------------------\n",
      "343 M     Trainable params\n",
      "0         Non-trainable params\n",
      "343 M     Total params\n",
      "1,373.321 Total estimated model params size (MB)\n",
      "Epoch 0:   0%|                                          | 0/228 [00:00<?, ?it/s]/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "tensor(1.3880)\n",
      "Epoch 0:   0%|                        | 1/228 [00:06<25:02,  0.15it/s, v_num=58]tensor(1.3858)\n",
      "Epoch 0:   1%|▏                       | 2/228 [00:15<29:26,  0.13it/s, v_num=58]tensor(1.3839)\n",
      "Epoch 0:   1%|▎                       | 3/228 [00:23<29:27,  0.13it/s, v_num=58]tensor(1.3895)\n",
      "^C\n",
      "/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "!python main.py fit \\\n",
    "    --trainer.accelerator \"cpu\" \\\n",
    "    --data.train_data_path \"dataset/SP-train.npy\" \\\n",
    "    --model.gradient_checkpointing True \\\n",
    "    --trainer.max_epochs 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/fabric/utilities/seed.py:40: No seed found, seed set to 3337049792\n",
      "Seed set to 3337049792\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type              | Params\n",
      "---------------------------------------------------------\n",
      "0 | encoder            | T5EncoderModel    | 341 M \n",
      "1 | choices_classifier | Sequential        | 2.1 M \n",
      "2 | train_f1           | MulticlassF1Score | 0     \n",
      "3 | valid_f1           | MulticlassF1Score | 0     \n",
      "---------------------------------------------------------\n",
      "343 M     Trainable params\n",
      "0         Non-trainable params\n",
      "343 M     Total params\n",
      "1,373.321 Total estimated model params size (MB)\n",
      "Sanity Checking DataLoader 0:   0%|                       | 0/1 [00:00<?, ?it/s]/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/allennlp_light/nn/util.py:1521: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  return torch.cuda.LongTensor(size, device=device).fill_(1).cumsum(0) - 1\n",
      "terminate called after throwing an instance of 'c10::Error'                     \n",
      "  what():  CUDA error: initialization error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:44 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f8c39f0f617 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f8c39eca98d in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f8c39fcb128 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)\n",
      "frame #3: c10::cuda::ExchangeDevice(int) + 0x8a (0x7f8c39fcb58a in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)\n",
      "frame #4: <unknown function> + 0xe24954 (0x7f8bc5424954 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #5: <unknown function> + 0x513d26 (0x7f8c07513d26 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #6: <unknown function> + 0x55ca7 (0x7f8c39ef4ca7 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #7: c10::TensorImpl::~TensorImpl() + 0x1e3 (0x7f8c39eeccb3 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #8: c10::TensorImpl::~TensorImpl() + 0x9 (0x7f8c39eece49 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #9: <unknown function> + 0x7c8698 (0x7f8c077c8698 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #10: THPVariable_subclass_dealloc(_object*) + 0x305 (0x7f8c077c8a25 in /home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\n",
      "<omitting python frames>\n",
      "\n",
      "/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "Epoch 0:   0%|                                            | 0/9 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1132, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/queue.py\", line 180, in get\n",
      "    self.not_empty.wait(remaining)\n",
      "  File \"/usr/lib/python3.11/threading.py\", line 331, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 22764) is killed by signal: Aborted. \n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arch/Projects/college/semeval_brainteaser/main.py\", line 12, in <module>\n",
      "    cli_main()\n",
      "  File \"/home/arch/Projects/college/semeval_brainteaser/main.py\", line 9, in cli_main\n",
      "    cli = LightningCLI(MultipleChoicesModel, SemevalDataModule)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 386, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 677, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 544, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 44, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 580, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 989, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 1035, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 202, in run\n",
      "    self.advance()\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 359, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 136, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 202, in advance\n",
      "    batch, _, __ = next(data_fetcher)\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/fetchers.py\", line 127, in __next__\n",
      "    batch = super().__next__()\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/fetchers.py\", line 56, in __next__\n",
      "    batch = next(self.iterator)\n",
      "            ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/utilities/combined_loader.py\", line 326, in __next__\n",
      "    out = next(self._iterator)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/utilities/combined_loader.py\", line 74, in __next__\n",
      "    out[i] = next(self.iterators[i])\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1328, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1284, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1145, in _try_get_data\n",
      "    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e\n",
      "RuntimeError: DataLoader worker (pid(s) 22764) exited unexpectedly\n",
      "Epoch 0:   0%|          | 0/9 [00:11<?, ?it/s]                                  \n"
     ]
    }
   ],
   "source": [
    "!python main.py fit \\\n",
    "    --trainer.accelerator \"gpu\" \\\n",
    "    --data.debug True \\\n",
    "    --data.train_data_path \"dataset/SP-train.npy\" \\\n",
    "    --model.gradient_checkpointing True \\\n",
    "    --trainer.max_epochs 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Seed set to 69\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arch/Projects/college/semeval_brainteaser/src/train.py\", line 37, in <module>\n",
      "    main(args)\n",
      "  File \"/home/arch/Projects/college/semeval_brainteaser/src/train.py\", line 20, in main\n",
      "    trainer = L.Trainer.from_argparse_args(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: type object 'Trainer' has no attribute 'from_argparse_args'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Seed set to 69\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type              | Params\n",
      "---------------------------------------------------------\n",
      "0 | encoder            | T5EncoderModel    | 341 M \n",
      "1 | choices_classifier | Sequential        | 1.1 M \n",
      "2 | train_f1           | MulticlassF1Score | 0     \n",
      "---------------------------------------------------------\n",
      "342 M     Trainable params\n",
      "0         Non-trainable params\n",
      "342 M     Total params\n",
      "1,369.127 Total estimated model params size (MB)\n",
      "Epoch 0:   0%|                                          | 0/396 [00:00<?, ?it/s]/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/allennlp_light/nn/util.py:1521: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  return torch.cuda.LongTensor(size, device=device).fill_(1).cumsum(0) - 1\n",
      "tensor(0.3467, device='cuda:0')\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arch/Projects/college/semeval_brainteaser/src/train.py\", line 38, in <module>\n",
      "    main(args)\n",
      "  File \"/home/arch/Projects/college/semeval_brainteaser/src/train.py\", line 26, in main\n",
      "    trainer.fit(model, data_loader)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 544, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 44, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 580, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 989, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 1035, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 202, in run\n",
      "    self.advance()\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 359, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 136, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 240, in advance\n",
      "    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 187, in run\n",
      "    self._optimizer_step(batch_idx, closure)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 265, in _optimizer_step\n",
      "    call._call_lightning_module_hook(\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/core/module.py\", line 1291, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py\", line 151, in step\n",
      "    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 230, in optimizer_step\n",
      "    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py\", line 117, in optimizer_step\n",
      "    return optimizer.step(closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 373, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/optim/adamw.py\", line 173, in step\n",
      "    self._init_group(\n",
      "  File \"/home/arch/.pyenv/versions/semeval/lib/python3.11/site-packages/torch/optim/adamw.py\", line 121, in _init_group\n",
      "    state[\"exp_avg\"] = torch.zeros_like(\n",
      "                       ^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 3.81 GiB of which 4.88 MiB is free. Process 9867 has 50.00 MiB memory in use. Including non-PyTorch memory, this process has 3.75 GiB memory in use. Of the allocated memory 3.66 GiB is allocated by PyTorch, and 15.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Epoch 0:   0%|          | 0/396 [00:02<?, ?it/s]                                \n"
     ]
    }
   ],
   "source": [
    "!python src/train.py \\\n",
    "    --data_path dataset/WP-train.npy \\\n",
    "    --accelerator gpu \\\n",
    "    --devices 1 \\\n",
    "    --batch_size 1 \\\n",
    "    --gradient_checkpointing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
